# FTâ€¯Newsâ€¯ScraperÂ ğŸ•·ï¸

Scrapes Financial Times **World** headlines with ScrapyÂ +Â Playwright and saves structured articles to a Postgres database (running in Docker).

---

##â€¯Prerequisites

| Tool | VersionÂ (tested) |
|------|------------------|
| **DockerÂ Engine** | â‰¥Â 24â€¯CE |
| **DockerÂ Compose** | v2Â plugin (bundled with DockerÂ Desktop) |
| **Python** | 3.11 (only if you run helper scripts outside Docker) |

---

##â€¯1.Â CloneÂ &Â install

```bash
git clone git@github.com:your-org/ft_scraper.git
cd ft_scraper
cp .env.example .env          # fill in secrets â€“ see next section
```

---

##â€¯2.Â Configure secrets (`.env`)

```dotenv
#Â Postgres
POSTGRES_DB=ft_scraper
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=scraper_pass
POSTGRES_HOST=db
POSTGRES_PORT=5432

#Â FT credentials for Playwright cookie harvester
EMAIL=you@example.com
PASSWORD=********
```

*Never commit real credentials â€“ `.env` is gitâ€‘ignored.*

---

##â€¯3.Â Harvest FTÂ cookiesÂ (once)

```bash
# runs Playwright, logs in, saves ft_cookies.json
docker compose run --rm scraper python ft_scraper/ft_cookies_farming.py
```

A fileÂ `ft_scraper/ft_cookies.json` is created and will be picked up by the spider.

---

##â€¯4.Â Bootstrap runÂ (30Â days history)

```bash
FIRST_RUN=yes docker compose up --build         # first time only
```

* The flag `FIRST_RUN=yes` tells the spider to crawl the last **30â€¯days**.
* Data land in the Postgres tableÂ `articles`; duplicates are ignored.

When the bootstrap finishes, **remove** or set `FIRST_RUN=no` in
`docker-compose.yml`â€¯/â€¯your CIÂ environment.

---

##â€¯5.Â Incremental runsÂ (hourly news)

```bash
docker compose up -d db           # ensure DB container is running
docker compose up -d scraper      # spider now scrapes only the last hour
```

To keep it continuous you can:

* wrap the scraper container in a `while true; sleep 3600; ...`
  entrypoint **or**
* schedule `docker compose run --rm scraper scrapy crawl ftSpider` via cron.

---

##â€¯6.Â Querying the database

```bash
docker compose exec db psql -U $POSTGRES_USER -d $POSTGRES_DB   -c "SELECT COUNT(*), MIN(published_at), MAX(published_at) FROM articles;"
```

---

##â€¯Project layout

```
ft_scraper/
â”œâ”€â”€ ft_scraper/
â”‚   â”œâ”€â”€ spiders/           # FtSpider lives here
â”‚   â”œâ”€â”€ pipelines.py       # Postgres upsert, table autoâ€‘create
â”‚   â”œâ”€â”€ ft_cookies_farming.py
â”‚   â””â”€â”€ data/              # scraped JSON feeds (gitâ€‘ignored)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```


