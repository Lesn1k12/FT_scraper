# FTÂ NewsÂ ScraperÂ ğŸ•·ï¸

Scrapes Financial Times **World** headlines with ScrapyÂ +Â Playwright and saves
structured articles to a Postgres database (running in Docker).

---

##Â Prerequisites

| Tool | Version (tested) |
|------|------------------|
| **DockerÂ Engine** | â‰¥Â 24Â CE |
| **DockerÂ Compose** | v2 plugin (bundled with Docker Desktop) |
| **Python** | 3.11 (only if you run helper scripts outside Docker) |

---

##Â 1.Â Clone & install

```bash
git clone git@github.com:<yourâ€‘org>/ft_scraper.git
cd ft_scraper
cp .env.example .env           # fill in secrets â€“ see next section

##Â 2.Â Configure secrets (.env)
# Postgres
POSTGRES_DB=ft_scraper
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=scraper_pass
POSTGRES_HOST=db
POSTGRES_PORT=5432

# FT credentials for Playwright cookie harvester
EMAIL=you@example.com
PASSWORD=********

##Â 3.Â Harvest FTÂ cookies (once)
# runs Playwright, logs in, saves ft_cookies.json
docker compose run --rm scraper python ft_scraper/ft_cookies_farming.py

A fileÂ ft_scraper/ft_cookies.json is created and will be picked up by the spider.

##Â 4.Â Bootstrap run (30Â days history)
FIRST_RUN=yes docker compose up --build         # first time only

The flagÂ FIRST_RUN=yes tells the spider to crawl the last 30Â days.

Data land in the Postgres tableÂ articles; duplicates are ignored.

When the bootstrap finishes, remove or set FIRST_RUN=no in
docker-compose.ymlâ€¯/â€¯your CIÂ environment.

##Â 5.Â Incremental runs (hourly news)
docker compose up -d db           # ensure DB container is running
docker compose up -d scraper      # spider now scrapes only the last hour

To keep it continuous you can:

wrap the scraper container in a while true; sleep 3600; ...
entrypoint or

schedule docker compose run --rm scraper scrapy crawl ftSpider via cron.

##Â 6.Â Querying the database
docker compose exec db psql -U $POSTGRES_USER -d $POSTGRES_DB \
  -c "SELECT COUNT(*), MIN(published_at), MAX(published_at) FROM articles;"
